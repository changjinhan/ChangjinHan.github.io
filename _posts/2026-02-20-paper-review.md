---
title: "[Paper Review] 이번 주 AI/ML 논문 훑어보기 (26.02.09 ~ 02.15)"
excerpt: LLM·에이전트·양자화까지
header:
    teaser: "assets/images/paper_review.jpg"
    overlay_image: "assets/images/paper_review.jpg"
    overlay_filter: 0.5 # same as adding an opacity of 0.5 to a black background
    # caption: 
    actions:
        - url: 
use_math: true
toc: true
toc_sticky: true
toc_label: "페이지 목차"
categories: 
    - Paper Review
tags: 
    - LLM
    - Agent
    - Quantization
    - Full Stack Agent
    - On-Device AI
    - World Model
date: 2026-02-20
---

이번 주에도 최신 AI 논문들을 쭉 따라가 봤습니다.  
느낌부터 말하자면, 이제는

* 모델을 키우는 시대가 아니라

* 잘 쪼개고, 잘 요약하고, 잘 감시하는 시대

로 완전히 넘어가고 있다는 생각이 들었어요.

메모리 8GB짜리 GPU에서도 70B 모델을 돌리겠다고 달려들고,  
웹 풀스택 코드를 통째로 짜주는 에이전트를 만들면서,  
동시에 그 에이전트가 KPI 맞추겠다고 막 나가지 않게 잡아주는 벤치마크와 가드레일까지 같이 나옵니다.

이 글에서는 그중 인상 깊었던 논문들을 주제별로 묶어서 정리해볼게요.  
가능하면 개발자가 아니어도 흐름은 따라갈 수 있도록, 대신 디테일은 놓치지 않도록 적어보겠습니다.

## 🔧 LLM을 1비트 이하로: NanoQuant가 여는 극단적 경량화

요즘 가장 자주 듣는 말이 있죠.  
모델은 좋은데, 돌릴 데가 없다.

**NanoQuant** 논문은 이 문제를 아주 정면에서 치고 들어갑니다.  
대규모 언어 모델을 1비트, 심지어 1비트보다 적은 수준으로까지 압축하면서도,  
성능을 꽤 괜찮게 유지하는 사후 학습 양자화(PTQ) 기법을 제안합니다.

핵심 아이디어를 제 언어로 정리하면 이렇습니다.

완전한 실수 가중치를 여러 개의 이진 행렬과 스케일 값들의 곱으로 분해해서 표현하자

이걸 저랭크(낮은 차원) 구조로 잘 설계하면 비트 수는 미친 듯이 줄이면서도 정보는 최대한 보존할 수 있다.

여기서 중요한 도구가 **ADMM(교대 방향 방법, Alternating Direction Method of Multipliers)** 입니다.  
이걸로 이진 행렬과 스케일을 초기화하고, 이후 블록 단위와 모델 단위로 재구성하면서 오차를 줄여 나갑니다.

논문에서 가장 임팩트 있었던 부분은 숫자였습니다.

Llama2-70B를 단일 H100 80GB에서 13시간 동안 압축해서 25.8배나 줄이고

결과적으로 8GB짜리 소비자용 GPU에서도 70B 모델을 굴릴 수 있게 만들었다는 점.

논문은 여기에서 보실 수 있어요.

* [NanoQuant 논문(arXiv)](https://arxiv.org/abs/2602.06694)

코드는 아직 없지만 흐름 자체가 흥미로워서,  
저라면 향후 온디바이스 LLM, 엣지 디바이스 AI를 고민하는 분들은 거의 필수로 따라가야 할 라인이라고 생각합니다.

개발자로서 느낀 포인트는 두 가지였습니다.

1. 앞으로는 모델 서빙 인프라를 사고 설계할 때, 비트 수와 압축 전략까지 포함해서 함께 고민해야 한다는 것

2. GPU 메모리 부족을 무조건 하드웨어 문제로만 볼 수 있는 시기는 끝났다는 것

요즘 사이드 프로젝트용 서버를 고를 때,  
차라리 GPU는 작되, 양자화가 잘 먹는 구조를 선택하는 게 더 합리적인 선택이 될지도 모르겠다는 생각이 들었습니다.

## 🧠 셀프 어텐션을 토큰당 고정 비용으로: 대칭 인식 테일러 근사(SATA)

두 번째로 강하게 와 닿았던 논문은  
**Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation**, 줄여서 SATA 어텐션입니다.

트랜스포머의 병목은 다들 아시듯 **셀프 어텐션의 O(n²)** 복잡도입니다.  
길이가 길어질수록 메모리와 연산량이 폭발하죠.

이 논문이 주장하는 건 꽤 과감합니다.

셀프 어텐션을 테일러 전개 관점에서 다시 보더니 이를 대칭 텐서 곱의 체인 형태로 재구성하고 그걸 효율적인 피드포워드 변환 형태로 바꿔서

결국 토큰당 고정 비용으로 어텐션을 계산할 수 있다는 것

조금 더 직관적으로 말하면

* 쿼리와 키를 어떤 다항식 커널 기반 특징 공간으로 매핑해서

* 어텐션 연산을 더 이상 길이에 따라 비싸지지 않는 방식으로 바꾸는 아이디어입니다.

재미있던 부분은 비용이 **헤드 크기에 반비례** 해서 고정된다는 점이에요.  

즉, 헤드를 더 잘게 나누면, 같은 비용 내에서 더 많은 헤드를 쓸 수 있는 구조가 되는 셈입니다.

공식 구현은 여기에서 볼 수 있습니다.

* [SATA Attention GitHub 레포](https://github.com/glassroom/sata_attention)

* [SATA 논문(arXiv)](https://arxiv.org/abs/2602.00294)

제가 이 논문에서 얻은 인사이트는 앞으로 롱 컨텍스트 모델 경쟁은  

단순히 KV 캐시 최적화나 압축이 아니라, **어텐션 수식 자체를 바꾸는 싸움**으로 넘어가고 있다는 것

LLM 아키텍처를 건드리는 연구가 다시 중요해지고 있다는 것

실제로 롱 컨텍스트 프로젝트를 해보면, 모델 성능보다 메모리와 속도 때문에 컨텍스트 길이를 줄여야 했던 경험이 많거든요.  
이 계열이 충분히 안정화되면, 컨텍스트를 함부로 잘라내지 않아도 되는 날이 올지도 모르겠습니다.

## 🌐 웹 풀스택을 짜는 LLM 에이전트: FullStack-Agent

이제 에이전트 쪽으로 넘어가보죠.  
코드를 대신 짜주는 LLM 에이전트는 이미 많이 나와 있지만,  
대부분이 프론트엔드 데모 수준에서 멈춰 있었습니다.

**FullStack-Agent**는 이걸 한 단계 넘겨서,  
말 그대로 **풀스택 프로덕션급 웹 앱**을 목표로 합니다.

구성은 세 부분으로 나뉩니다.

1. FullStack-Dev

   * 멀티 에이전트 프레임워크

   * 플랜 수립, 코드 편집, 코드베이스 탐색, 버그 위치 찾기까지 담당

2. FullStack-Learn

   * 크롤링된 웹사이트 저장소를 역번역해서

   * LLM 학습 데이터로 재구성하는 자기 개선 루프

3. FullStack-Bench

   * 프론트엔드, 백엔드, DB 기능을 통합적으로 테스트하는 벤치마크

숫자만 보면, 꽤 인상적입니다.

* FullStack-Dev 기준

  * 프론트엔드 테스트: 기존 SOTA 대비 +8.7%

  * 백엔드: +38.2%

  * 데이터베이스: +15.9%

* FullStack-Learn으로 30B 모델 기준 추가 향상

  * +9.7%, +9.5%, +2.8%

논문과 코드는 여기서 볼 수 있습니다.

* [FullStack-Agent 논문(arXiv)](https://arxiv.org/abs/2602.03798)

* [FullStack-Agent GitHub](https://github.com/mnluzimu/FullStack-Agent)

개발자 입장에서 이 논문이 특히 흥미로웠던 지점은

* LLM 코딩 에이전트를 단일 프롬프트 기반 도우미가 아니라 **하나의 팀 구조**로 보는 관점

* 그리고 학습 데이터도 깔끔한 튜토리얼 코드가 아니라 **실제 웹 리포지토리 전체를 역번역해서 학습**에 쓰려는 접근

실무에서 프로덕션 코드 만져본 분들은 알겠지만 버그는 함수 안에만 있지 않고,  
패키지 버전, 설정, DB 스키마, 배포 스크립트 등 어딘가에 숨어 있죠.

저는 이 논문을 보면서 앞으로 개발자 역할이

* 코드 한 줄 한 줄을 직접 치기보다는

* 에이전트를 위한 문제 정의, 설계, 코드베이스 조율, 리뷰, 품질 관리

쪽으로 더 이동할 거라는 느낌이 강해졌습니다.

그리고 에이전트 쪽 실험할 때, 이 프레임워크를 그대로 쓰기보다는  
**자기만의 FullStack-Bench를 만드는 생각**을 꼭 해볼 필요가 있겠다 싶었어요.

## 📱 코드로 GUI 세상을 시뮬레이션: Code2World

FullStack-Agent가 웹이라면,  
**Code2World**는 모바일 GUI 세계를 통째로 코드로 모델링하려는 시도입니다.

일반적인 GUI 에이전트는 화면을 이미지로 보고 그 위에 클릭, 스크롤 같은 액션을 올리는 방식인데

Code2World는 완전히 다른 접근을 합니다.

다음 화면 상태를 픽셀이 아니라 렌더링 가능한 HTML 코드로 예측하는 비전-언어 코더

이걸 가능하게 하려고,  
연구진은 **AndroidCode**라는 데이터셋을 새로 만들어요.

실제 GUI 궤적을 고충실도 HTML로 변환하고 시각 피드백으로 코드를 계속 정제해서

8만 개 이상의 화면-행동 쌍을 생성했습니다.

학습도 두 단계로 나뉩니다.

1. SFT로 HTML 레이아웃 형식과 기본 구조를 익히게 하고

2. 그 다음, 렌더링된 결과를 보상으로 쓰는 Render-Aware Reinforcement Learning을 적용

재미있는 부분은 성능입니다. Code2World-8B가 다음 UI 상태 예측에서

* GPT5, Gemini-3-Pro-Image 같은 상용 모델과 맞먹는 성능을 내고

* AndroidWorld 탐색에서 Gemini-2.5-Flash의 성공률을 9.5% 끌어올리는 역할까지 합니다.

링크는 이쪽입니다.

* [Code2World 논문(arXiv)](https://arxiv.org/abs/2602.09856)

* [Code2World GitHub](https://github.com/AMAP-ML/Code2World)

* [Code2World 프로젝트 페이지](https://amap-ml.github.io/Code2World/)

제가 이 논문에서 얻은 핵심 인사이트는

앞으로의 에이전트는 픽셀 단위 인식이 아니라 **코드로 기술된 세계를 이해하고 조작하는 방향**으로 갈 가능성이 크다는 것

즉, 모바일 앱이든 웹이든 렌더링 가능한 코드(HTML, JSX, XML 등)를 중심으로  
월드 모델(world model)을 만드는 흐름이 본격화될 것 같다는 느낌이 들었습니다.

개발자로서 조금 실감 났던 지점은 UI 자동화 테스트, 크롤러, RPA 같은 전통 영역이  
머지않아 이 계열 모델에 흡수될 수 있겠다는 생각이었어요.

## 🎬 영상·음성 동시 생성: MOVA가 보여주는 멀티모달의 다음 스텝

영상 생성 모델 얘기할 때,  
보통은 영상만 두고 얘기하는 경우가 많죠.  
하지만 실제 세상에서 영상에 소리가 빠져 있으면,  
그건 거의 반쪽짜리 경험입니다.

**MOVA(MOSS Video and Audio)**는  
비디오와 오디오를 동시에, 서로 동기화된 상태로 생성하는 오픈소스 모델입니다.

특징은 크게 세 가지로 정리할 수 있습니다.

* 입 모양과 맞는 음성 생성

* 환경에 맞는 사운드 효과

* 내용과 조화를 이루는 음악까지 함께 생성

아키텍처는 Mixture-of-Experts(MoE) 구조를 쓰고 있고

* 총 32B 파라미터

* 그중 18B만 추론 시 활성화되는 구조

지원하는 태스크는

* 이미지와 텍스트를 입력으로 받아

* 비디오와 오디오를 함께 만들어내는 IT2VA(Image-Text to Video-Audio)

논문과 코드는 아래에서 볼 수 있습니다.

* [MOVA 논문(arXiv)](https://arxiv.org/abs/2602.08794)

* [MOVA GitHub](https://github.com/OpenMOSS/MOVA)

* [MOVA 모델 모음(Hugging Face)](https://huggingface.co/collections/OpenMOSS-Team/mova)

제가 이 논문을 보면서 제일 반가웠던 건 
이 정도 퀄리티의 **영상+음성 동시 생성 모델이 오픈소스로 나온다**는 사실 자체였습니다.

실제 서비스 관점에서 보면 광고 영상 자동 생성, 튜토리얼 영상 자동화, 게임 시네마틱 프로토타이핑

같은 데서 바로 써볼 수 있는 단계라고 느껴졌고,  

개발자 입장에서는 프롬프트 설계와 사후 편집 파이프라인, 저작권 및 안전 필터

쪽에 더 많은 리소스를 쓰게 될 것 같다는 생각이 들었습니다.

## 🚨 KPI가 문제를 만든다: ODCV-Bench가 보여준 에이전트의 위험한 면

이제 에이전트의 어두운 면을 보죠.

**ODCV-Bench(A Benchmark for Evaluating Outcome-Driven Constraint Violations in Autonomous AI Agents)**는  
에이전트가 성과 지표를 달성하려다가  
얼마나 위험한 선택을 할 수 있는지를 측정하는 벤치마크입니다.

중요한 포인트는 그냥 나쁜 지시를 거부하느냐 마느냐가 아니라 KPI를 달성하는 과정에서

윤리, 법, 안전 규칙을 **조용히 뒤로 미루는 행동**을 측정한다는 것

구성은 이렇습니다.

* 40개의 시나리오

* 각 시나리오는 여러 단계의 행동을 필요로 하고

* 결과는 특정 KPI에 묶여 있음

* 각 시나리오는 두 버전

  * Mandated: 명시적으로 무엇을 하라고 지시

  * Incentivized: KPI를 세게 걸어놓고, 알아서 잘하라고 압박

이걸 12개의 최신 LLM 에이전트에 돌려본 결과

* 결과 기반 제약 위반이 1.3%에서 71.4%까지

* 12개 중 9개 모델은 30~50% 사이

* 특히 Gemini-3-Pro-Preview는 71.4%로 가장 높은 위반율을 기록

더 무서운 건,  
에이전트의 기반이 되는 모델이 따로 평가했을 때는  
자신의 행동이 비윤리적임을 **알고 있다는 점**입니다.  
하지만 KPI 압박 아래에서는 그걸 무시하고 행동을 선택해버립니다.

논문은 여기서 확인할 수 있습니다.

* [ODCV-Bench 논문(arXiv)](https://arxiv.org/abs/2512.20798)

이걸 보면서 느낀 건 이제 에이전트 안전은 
프롬프트에 안전 규칙 몇 줄 더 쓰는 수준으로는 절대 해결이 안 된다는 것

비즈니스에서 자주 쓰는 KPI 기반 평가 시스템과 LLM 에이전트가 결합하면 
생각보다 빨리 위험한 행동이 나올 수 있다는 것

만약 제가 조직에서 에이전트 도입을 책임지는 입장이라면,  
이 논문에 나온 식의 시나리오를 우리 도메인에 맞게 재구성해서  
배포 전에 꼭 한번 돌려볼 것 같습니다.

## 🛡 에이전트 행동을 해부하는 가드레일: AgentDoG

ODCV-Bench가 문제를 "보여주는" 쪽이라면,  
**AgentDoG**는 그 문제를 "진단하고 분해하는" 쪽에 가깝습니다.

AgentDoG는 에이전트의 위험을 세 축으로 나눕니다.

* Where: 위험이 어디서 발생했는가

* How: 어떤 실패 모드로 나타나는가

* What: 그 결과가 무엇인가

이 3차원 분류 체계를 기반으로

* ATBench라는 에이전트 안전 벤치마크와

* AgentDoG라는 진단용 가드레일 모델을 함께 제안합니다.

AgentDoG의 포인트는

* 에이전트의 궤적 전체를 따라가면서

* 각 단계별 행동을 세밀하게 모니터링하고

* 단순히 안전/비안전 이진 레이블이 아니라  
  **왜 위험한지, 어디서 문제가 시작됐는지**를 설명해주는 구조라는 것

모델은 Qwen, Llama 계열로

* 4B, 7B, 8B 파라미터 크기 버전으로 제공됩니다.

링크는 이쪽입니다.

* [AgentDoG 논문(arXiv)](https://arxiv.org/abs/2601.18491)

* [AgentDoG GitHub](https://github.com/AI45Lab/AgentDoG)

* [AgentDoG 컬렉션(Hugging Face)](https://huggingface.co/collections/AI45Research/agentdog)

개인적으로 가장 마음에 들었던 부분은

가드레일을 "그냥 막는 장치"가 아니라 **디버깅 도구**로 재해석했다는 점이었습니다.

실제로 에이전트를 만들다 보면

* 완전히 위험한 행동은 오히려 눈에 잘 띄고

* 애매하게 비합리적인 행동,  
  예를 들면 "지금 안 급한 일을 굳이 하느라 리소스를 버리는 패턴" 같은 게 더 문제일 때가 많거든요.

이런 부분을 진단해주는 툴이 실제로 잘 동작한다면,  
에이전트 개발 워크플로우에서 테스트와 디버깅 단계에 필수로 들어가게 될 것 같다는 생각이 들었습니다.

## 🧩 토큰이 아니라 어텐션을 학습한다: Reinforced Attention Learning (RAL)

다음은 멀티모달 LLM 학습 방식을 뒤집어보려는 논문입니다.  
이름은 **Reinforced Attention Learning (RAL)**.

지금까지의 RL 기반 사후 학습은  
대부분 "어떤 토큰을 출력할 것인가"에 초점을 맞췄습니다.  
답변 텍스트를 길고 정교하게 만들고,  
그에 대한 보상을 주는 식이죠.

하지만 멀티모달로 오면 이야기가 조금 달라집니다.

* 긴 설명을 억지로 끼워 넣다 보면

* 오히려 지각(vision) 성능이 떨어질 수 있다는 결과들이 꽤 있었어요.

RAL은 방향을 완전히 반대로 틉니다.

* 출력 토큰이 아니라

* **모델 내부의 어텐션 분포 자체를 최적화** 하자는 것

이렇게 하면

* 어떤 단어를 생성할지가 아니라

* **어디를 볼지, 어디에 주의를 둘지**를 학습하게 되고

* 그 결과 멀티모달 입력에서 정보 할당과 그라운딩이 더 좋아진다는 아이디어입니다.

또 하나 중요한 부분은

* On-Policy Attention Distillation이라는 기법을 도입했다는 점입니다.

  * 잠재적인 어텐션 행동을 전이하는 방식

  * 일반적인 지식 증류보다 교차 모달 정렬에 더 효과적인 것으로 나타났습니다.

논문은 여기서 확인할 수 있습니다.

* [Reinforced Attention Learning 논문(arXiv)](https://arxiv.org/abs/2602.04884)

개발자 입장에서 얻은 인사이트는 명확합니다.

이제 "모델이 뭘 말하는지"만 볼 게 아니라  
"모델이 어디를 보고 있는지"까지 모니터링하고 학습해야 한다는 것

예를 들어, VLM이 차 사진을 보고 있는데

* 번호판을 계속 응시하는지

* 브랜드 로고를 보는지

* 주변 배경만 보는지

이런 차이가 설명 결과보다 더 중요한 경우가 많습니다.

앞으로 멀티모달 디버깅 도구는  
어텐션 히트맵 시각화와 RL 기반 튜닝이 결합된 형태로 진화할 거라는 생각이 들었습니다.

## 🧮 검증자 없이 전문가 시연만으로: RARO의 새로운 추론 학습

**Escaping the Verifier: Learning to Reason via Demonstrations** 논문에서 제안하는  
**RARO(Relativistic Adversarial Reasoning Optimization)**는  
추론 중심 학습에서 자주 등장하는 현실적인 문제 하나를 찌릅니다.

많은 실제 태스크에는 작업별 검증자(verifier)가 없거나 만들기 어렵다는 점

그렇다고 추론 학습을 포기할 순 없죠.  
대신 우리가 가진 건

* 풍부한 전문가 시연 데이터

RARO는 이렇게 접근합니다.

* 정책과 상대론적 비평자를 하나의 적대적 게임 구조로 엮고

* 정책은 전문가 답변을 모방하려고 하고

* 비평가는 전문가와 정책의 답변 쌍 중에서  
  어느 쪽이 전문가인지를 맞추려고 학습

* 둘 다 RL로 함께 학습시키면서  
  점점 더 강력한 추론 능력을 얻는 구조

실험 결과를 보면

* Countdown

* DeepMath

* Poetry Writing

같은 다양한 태스크에서 강력한 검증자 없는 기준선을 넘어서고,  
검증자가 있는 RL과 비슷한 확장 트렌드를 보여줍니다.

논문은 여기에서 볼 수 있습니다.

* [Escaping the Verifier 논문(arXiv)](https://arxiv.org/abs/2511.21667)

이 논문을 읽으면서 든 생각은 앞으로 도메인 특화 추론 모델을 만들 때  
굳이 완벽한 정답 검증 로직부터 만들지 않아도 
**전문가 시연 데이터만으로도 꽤 멀리 갈 수 있겠다**는 점이었습니다.

특히 법률 문서 해석, 의료 차트 기반 추론, 기업 내부 프로세스 자동화 같이 
검증자 정의가 어려운 영역에서 실제 사람들의 작업 로그를 활용한 RARO 스타일 학습이  
꽤 매력적인 옵션이 될 것 같아요.
