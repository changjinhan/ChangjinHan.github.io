---
title: "[Paper Review] 이번 주 AI/ML 논문 훑어보기 (26.01.26 ~ 02.01)"
excerpt: 에이전트 조직·테스트타임 학습·소형 모델 반란까지
header:
    teaser: ""
    overlay_image: ""
    overlay_filter: 0.5 # same as adding an opacity of 0.5 to a black background
    # caption: 
    actions:
        - url: 
use_math: true
toc: true
toc_sticky: true
toc_label: "페이지 목차"
categories: 
    - Paper Review
tags: 
    - SLM
    - Positional Embedding
    - LLM
    - Agent
    - Test-Time Training
date: 2026-02-02
---

이번 주는 유난히 느낌이 강했던 주제들이 몇 개 있었습니다.

단일 LLM을 잘 키우는 시대에서, 여러 에이전트를 조직처럼 운영하고, 추론 중에 스스로 학습하고, 심지어 350M짜리 소형 모델로 대형 모델을 이기는 흐름까지 한 번에 몰려왔거든요.

이 글에서는 파이토치KR에서 정리된 2026년 1월 26일~2월 1일 AI/ML 논문 모음 중에서, 개인적으로 인상 깊었던 것들을 제 나름의 언어로 풀어보려고 합니다.

개념 설명 위주가 아니라, 실제로 지금 우리가 어떤 시스템을 만들 수 있는지, 개발자이자 사용자 입장에서 어떤 시사점이 있는지를 중심으로 이야기해볼게요.


## 🧠 단일 모델을 넘어서: 조직적 지능이라는 새로운 시야

이번 묶음에서 가장 먼저 눈에 들어온 흐름은,\
AI를 더 이상 하나의 모델로 보지 않고, **하나의 조직**으로 보기 시작했다는 점입니다.

대표적인 게 이 논문입니다.

* **If You Want Coherence, Orchestrate a Team of Rivals**\
  [https://arxiv.org/abs/2601.14351](https://arxiv.org/abs/2601.14351)

핵심 아이디어는 생각보다 단순하면서도 강력합니다.

* 한 개의 똑똑한 모델에 올인하지 말고

* 서로 다른 역할과 인센티브를 가진 여러 에이전트를 팀으로 묶어서

* 인간 회사 조직처럼 운영하자

논문에서 제안하는 구조를 사람 조직에 비유하면 대충 이런 느낌입니다.

* 기획자 에이전트: 문제를 쪼개고 전략을 짬

* 실행자 에이전트: 실제로 코드를 짜거나 도구를 호출

* 비평가 에이전트: 결과를 두드려 보고 문제를 찾아냄

* 전문가 에이전트: 특정 도메인(법률, 보안, 통계 등)에 대해 검수

여기서 특히 재밌었던 부분은,

에이전트들이 **서로 상반된 인센티브**를 갖도록 설계했다는 점입니다.\
다들 같은 목표(좋은 결과)를 바라보지만, 한 에이전트는 문제를 많이 찾을수록 보상을 받고, 다른 에이전트는 가능한 한 빠르게 일을 끝내는 쪽으로 보상을 받는 식이죠.

이 구조 덕분에, 사용자에게 보여주기 전에 내부에서 **90% 이상의 오류를 잡았다**는 결과가 나옵니다.

개발자로서 이걸 보면서 든 생각은 두 가지였습니다.

첫째, 이제 진짜로 "모델을 잘 고르는 것" 보다 "에이전트 조직을 어떻게 설계하고 운영하느냐"가 더 중요한 시대로 넘어가는 것 같습니다.

둘째, 우리가 지금 만드는 AI 서비스에 바로 적용할 수 있는 구조라는 점입니다.\
LLM을 여러 개 섞을 필요도 없고, 하나의 모델을 여러 역할 프롬프트로 띄워서 조직처럼 운영하는 것만으로도 꽤 많은 걸 할 수 있거든요.

에이전트 기반 시스템에 관심 있다면, 이 논문은 일단 북마크 해두고 천천히 뜯어볼 만합니다.



## 🤖 GPU만 늘리면 되는 시대는 끝: LLM 추론 하드웨어의 현실

대형 모델을 쓰다 보면, "이건 더 이상 연산량 문제가 아니라 메모리랑 대역폭 싸움이네" 하는 순간이 옵니다.

그 감각을 아주 정공법으로 정리한 논문이 바로 이거입니다.

* **Challenges and Research Directions for Large Language Model Inference Hardware**\
  [https://arxiv.org/abs/2601.05047](https://arxiv.org/abs/2601.05047)

요약하면, LLM 추론은 이제 이런 상태입니다.

* 학습은 대규모 연산이 핵심인데

* 추론은 **메모리 대역폭과 인터커넥트**가 병목

그래서 이 논문은 크게 네 가지 방향을 제시합니다.

1. HBM급 대역폭을 가지면서 용량은 10배 많은 플래시 기반 메모리

2. 메모리 근처 연산(Processing-Near-Memory)

3. 3D 메모리-로직 스택

4. 지연 시간이 낮은 고속 인터커넥트

제가 이걸 보면서 느낀 건,\
요즘 "소형 모델(SLM) 혁명"이 왜 이렇게 뜨거운지 하드웨어 관점에서 딱 설명이 된다는 점입니다.

* 하드웨어는 메모리 대역폭 때문에 고통받고

* 소프트웨어는 작은 모델+도구 조합으로 그걸 우회하려 하고

두 방향이 묘하게 맞물리고 있습니다.

즉, 당분간은 엄청 큰 단일 모델을 밀어붙이는 것보다

* 작고 잘 훈련된 모델

* 여기에 도구 호출, 캐싱, 에이전트 구조

* 그리고 하드웨어 친화적인 배치 설계

이 조합이 훨씬 실용적인 선택이 될 가능성이 큽니다.


## 🧩 350M짜리 모델이 툴콜링에서 ChatGPT를 이긴 이유

이 흐름을 가장 잘 보여주는 논문이 바로 이겁니다.

* **Small Language Models for Efficient Agentic Tool Calling**\
  [https://arxiv.org/abs/2512.15943](https://arxiv.org/abs/2512.15943)

이 논문이 주장하는 건 아주 직설적입니다.

* 특정 작업에 잘 맞게 파인튜닝된 소형 언어 모델이

* 툴 호출 같은 에이전트 작업에서는

* 대형 모델보다 **더 잘** 할 수 있다

여기서 쓰인 모델은 facebook/opt-350m, 즉 3억 5천만 파라미터짜리 정말 작은 모델입니다.

이걸 Hugging Face TRL(SFT)로 단 한 에폭 파인튜닝했는데, ToolBench 기준으로 이런 결과가 나옵니다.

* 파인튜닝된 SLM: 77.55%

* ChatGPT-CoT: 26.00%

* ToolLLaMA-DFS: 30.18%

* ToolLLaMA-CoT: 16.27%

솔직히 처음 이 수치를 봤을 때는 눈을 의심했습니다.\
"진짜 이렇게까지 차이가 난다고?"

하지만 내용을 읽다 보니 납득이 되는 부분이 있었는데요.

툴 호출은 사실상 이런 문제입니다.

* 어떤 도구를 언제 써야 하는지 결정

* 요구사항을 API 호출 형태로 잘 매핑

* 응답을 읽고 다시 다음 스텝을 계획

즉, **범용 지식**보다 "해당 도구 세트를 얼마나 잘 이해하고 있느냐"가 훨씬 중요합니다.

그걸 위해:

* 도메인에 맞는 데이터

* 깔끔한 툴 사용 예시

* 적절한 SFT 파이프라인

이 세 가지만 잘 설계하면, 굳이 수백억 파라미터짜리 모델을 쓸 이유가 점점 줄어드는 거죠.

개발자로서 제 인사이트는 하나입니다.

앞으로 에이전트 시스템을 설계할 때,

* 사용자 대화, 복잡한 추론: 중대형 LLM

* 툴 호출, 서빙, 라우팅: 작지만 잘 튜닝된 SLM

이렇게 역할을 분리하는 아키텍처가 점점 늘어날 것 같다는 점입니다.



## 🎻 ToolOrchestra: 8B 오케스트레이터가 GPT-5를 이겨버린 이유

툴 호출과 소형 모델 이야기를 했다면, 이 논문을 빼놓을 수 없습니다.

* **ToolOrchestra: Elevating Intelligence via Efficient Model and Tool Orchestration**\
  [https://arxiv.org/abs/2511.21689](https://arxiv.org/abs/2511.21689)

이 논문은 한마디로 정리하면 이렇습니다.

> 수많은 도구와 모델을 잘 조율하는 8B 오케스트레이터 하나가\
> GPT-5보다 싸고, 정확하고, 효율적일 수 있다

여기서 오케스트레이터는 이런 일을 합니다.

* 어떤 쿼리에 어떤 도구/모델을 쓸지 결정

* 툴 호출 순서를 설계

* 비용, 속도, 사용자 선호를 모두 고려해서 플랜 최적화

그리고 이 오케스트레이터를 강화학습으로 훈련합니다.

* 결과 품질

* 효율성(비용, 시간)

* 사용자 도구 선호

이 세 가지를 동시에 고려하는 보상을 설계해서요.

그 결과가 꽤 극적입니다.

* Humanity's Last Exam(HLE):\
  오케스트레이터 37.1% vs GPT-5 35.1%\
  게다가 2.5배 더 효율적

* tau2-Bench, FRAMES:\
  GPT-5를 크게 이기면서 비용은 약 30% 수준

실용적인 포인트는 이겁니다.

앞으로 AI 시스템 개발은

* 거대 모델 하나를 잘 쓰는 기술에서

* **도구·모델들을 어떻게 조합하고 오케스트레이션 할 것인가**로

무게중심이 확실히 옮겨갈 가능성이 크다는 점입니다.

관심 있다면 공식 페이지와 코드도 같이 보는 걸 추천합니다.

* 연구 페이지: [https://research.nvidia.com/labs/lpr/ToolOrchestra](https://research.nvidia.com/labs/lpr/ToolOrchestra)

* GitHub: [https://github.com/NVlabs/ToolOrchestra](https://github.com/NVlabs/ToolOrchestra)

* 오케스트레이터 모델: [https://huggingface.co/nvidia/Nemotron-Orchestrator-8B](https://huggingface.co/nvidia/Nemotron-Orchestrator-8B)

개인적으로는,\
앞으로 "AI 백엔드 개발자"라는 역할이 생긴다면 바로 이런 오케스트레이션 레이어를 설계하고 튜닝하는 사람이 아닐까 싶습니다.



## 🌀 Deep Delta Learning: 잔차 연결을 기하학적으로 다시 생각해보기

이제 조금 더 모델 내부 구조 이야기로 들어가 보죠.

* **Deep Delta Learning**\
  [https://arxiv.org/abs/2601.00417](https://arxiv.org/abs/2601.00417) \
  코드: [https://github.com/yifanzhang-pro/deep-delta-learning](https://github.com/yifanzhang-pro/deep-delta-learning)

이 논문은 우리가 너무 당연하게 받아들이던 **ResNet 스타일 잔차 연결**에 대해 꽤 도발적인 질문을 던집니다.

* 잔차 연결은 그래디언트 소실을 막는 데는 좋다

* 하지만 항상 "입력 + 변화량" 구조를 강요해서

* 복잡한 상태 전이를 표현하는 데 제약을 걸고 있다

그래서 이 논문은 **Delta Operator**라는 걸 도입합니다.

쉽게 말하면,

* 아이덴티티 행렬을 살짝 변형한

* 랭크 1짜리 기하학적 변환을

* 데이터에 따라 동적으로 학습하는 구조입니다.

이 덕분에 레이어가 할 수 있는 일이,

* 아무것도 안 하기(아이덴티티)

* 직교 투영

* 기하학적 반사

이 사이 어디든 부드럽게 위치할 수 있게 됩니다.

제가 재밌게 본 지점은 두 가지였습니다.

1. 이 연산자를 통해 **레이어별 전이 연산자의 스펙트럼**을 명시적으로 컨트롤할 수 있다는 점

2. 잔차 업데이트를 "랭크 1 주입" 관점에서 재해석했다는 점

실용적으로 당장 가져다 쓰기엔 약간 이른 감이 있지만,\
"잔차 연결도 결국 디자인 선택일 뿐"이라는 걸 다시 상기시켜주는 논문이라, 모델 아키텍처 쪽에 관심 있다면 한 번쯤 읽어볼 가치가 있습니다.



## 🧬 DroPE: 위치 임베딩을 버렸더니 문맥 길이가 늘어났다

긴 컨텍스트가 필요한 작업을 하다 보면 항상 부딪히는 벽이 있습니다.

"사전학습 최대 길이 이상은 그냥 안 된다"라는 그 벽이요.

이 논문은 그 벽을 꽤 우아한 방식으로 우회합니다.

* **Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings**\
  [https://arxiv.org/abs/2512.12167](https://arxiv.org/abs/2512.12167) \
  프로젝트 페이지: [https://pub.sakana.ai/DroPE](https://pub.sakana.ai/DroPE) \
  코드: [https://github.com/SakanaAI/DroPE](https://github.com/SakanaAI/DroPE)

핵심 아이디어는 제목 그대로입니다.

* 사전학습 때는 위치 임베딩이 수렴을 빠르게 도와준다

* 하지만 그 위치 정보에 너무 의존하면\
  → 보지 못한 길이의 시퀀스로 일반화가 안 된다

* 그래서, 사전학습이 끝난 뒤에\
  짧은 재조정 과정을 거쳐 **위치 임베딩을 통째로 제거**한다

놀라운 점은,

* 긴 컨텍스트에 대해 **제로샷으로 확장**되면서도

* 원래 길이에서의 성능이 크게 깨지지 않는다는 점입니다.

개인적으로 느낀 가장 큰 포인트는 이거였습니다.

> 우리가 "모델의 능력"이라고 믿고 있던 것 중 상당수가,\
> 사실은 "포지셔널 인코딩 설계의 부작용"일 수도 있겠구나

긴 컨텍스트 LLM을 좋아하거나,\
컨텍스트 윈도우 때문에 모델 교체를 고민하고 있다면\
DroPE는 진짜로 꼼꼼히 읽어볼 만한 논문입니다.



## 🎯 G²RL: LLM이 스스로 탐색 방향을 정하는 강화학습

강화학습으로 LLM을 튜닝할 때 항상 고민되는 게 있습니다.

* 엔트로피 보너스를 더 주면 다양성은 늘어나는데

* 그 다양성이 정말 "학습에 의미 있는 방향"인지 알 수 없다는 점입니다.

이 논문은 그 문제를 정면으로 파고듭니다.

* **Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning**\
  [https://arxiv.org/abs/2512.15687](https://arxiv.org/abs/2512.15687)

아이디어는 이름 그대로입니다.

> 탐색을 외부 휴리스틱에 맡기지 말고,\
> 모델 내부의 그래디언트 기하학이 직접 안내하게 하자

구체적으로는:

* 각 응답에 대해,\
  모델 마지막 레이어의 민감도를 기반으로 **시퀀스 단위 특징**을 만들고

* 한 번에 샘플링된 여러 응답의 이 특징들을 비교해서\
  "이 응답이 정책을 어느 방향으로 업데이트시킬지"를 추정

* 새로운 그래디언트 방향을 가져오는 응답에는 보상을 더 주고

* 기존 방향에 겹치거나 쓸모없는 업데이트는 덜 강조

이렇게 해서 탐색 신호를 만드는 프레임워크가 G²RL입니다.

실험은 Qwen3 1.7B, 4B 기반으로\
MATH500, AMC, AIME24/25, GPQA, MMLU-pro 등에서 진행됐고,\
전통적인 엔트로피 기반 방식보다 **pass@1, maj@16, pass@k**를 꾸준히 올렸습니다.

그래서 이 논문은 두 부류에게 특히 유용할 것 같습니다.

* RLHF, GRPO류를 직접 만져보는 분들

* "모델이 어떻게 배우는지"에 관심 있는 분들

저도 읽으면서 느낀 건,\
앞으로 LLM 강화학습은 점점 **모델 내부 기하학을 활용하는 방향**으로 갈 가능성이 크겠다는 점입니다.\
외부 임베딩이나 휴리스틱만으로는 한계가 명확하거든요.



## 🎥 Reward Forcing: 스트리밍 비디오에서 움직임 품질을 진짜로 끌어올린 방법

텍스트에서 살짝 벗어나서, 비디오 생성 쪽도 꽤 흥미로운 논문이 나왔습니다.

* **Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation**\
  [https://arxiv.org/abs/2512.04678](https://arxiv.org/abs/2512.04678) \
  모델: [https://huggingface.co/JaydenLu666/Reward-Forcing-T2V-1.3B](https://huggingface.co/JaydenLu666/Reward-Forcing-T2V-1.3B) \
  코드: [https://github.com/JaydenLyh/Reward-Forcing](https://github.com/JaydenLyh/Reward-Forcing)

문제 설정은 이렇습니다.

* 스트리밍 비디오 생성을 할 때

* 초기 프레임을 싱크 토큰처럼 계속 참고하다 보면

* 나중 프레임들이 움직임이 사라지고\
  "초기 프레임 복사본"처럼 되어버리는 현상

이 문제를 두 가지 축으로 해결합니다.

1. EMA-Sink 토큰

   * 초기 프레임에서 시작된 토큰을 유지하면서

   * 슬라이딩 윈도우 밖으로 나가는 토큰들을\
     지수 이동 평균으로 합쳐서 계속 업데이트

   * 추가 계산 없이 긴 문맥과 최신 동작을 동시에 담는 메모리 역할

2. Re-DMD(Rewarded Distribution Matching Distillation)

   * 교사 모델과의 분포 일치를 할 때

   * 모든 샘플을 똑같이 취급하지 않고

   * 비전-언어 모델이 "동작이 더 풍부하다"고 평가한 샘플에 더 가중치를 줌

   * 그래서 모델의 출력 분포를 "역동적인 비디오" 쪽으로 의도적으로 편향

실험 결과는 꽤 인상적입니다.

* 단일 H100에서 23.1 FPS 스트리밍

* 벤치마크 기준 SOTA 수준의 움직임 품질

제가 이 논문에서 얻은 인사이트는 하나입니다.

> 앞으로 멀티모달 생성은,\
> "좋은 평균"보다 "보상이 높은 구역에 맞춰 분포를 밀어붙이는 것"이\
> 점점 더 중요해질 거다

텍스트에서도 이미 RLHF, RLAIF 등이 그런 방향으로 가고 있는데,\
비디오 쪽에서도 비슷한 흐름이 시작됐다고 보면 될 것 같습니다.



## 🔁 Agent Drift: 다중 에이전트 시스템이 시간이 지나며 망가지는 방식

이 논문은 에이전트 시스템을 운영해 본 사람이라면\
한 번쯤은 체감했을 법한 문제를 아주 잘 짚어냅니다.

* **Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions**\
  [https://arxiv.org/abs/2601.04170](https://arxiv.org/abs/2601.04170)

논문에서 다루는 개념은 단 하나입니다.

> 에이전트 드리프트\
> = 여러 에이전트가 오랜 시간 상호작용하면서\
> 행동, 의사결정 품질, 상호 일관성이 서서히 무너지는 현상

이걸 세 가지로 나눠서 설명합니다.

* 의미 드리프트: 처음 의도에서 점점 조금씩 벗어나는 것

* 조정 드리프트: 에이전트 간 합의 메커니즘이 망가지는 것

* 행동 드리프트: 설계하지 않은 전략이 슬그머니 등장하는 것

그리고 이걸 정량화하기 위해 **Agent Stability Index(ASI)**라는 지표를 만듭니다.

여기에는 이런 차원이 포함됩니다.

* 응답 일관성

* 도구 사용 패턴

* 추론 경로 안정성

* 에이전트 간 동의율

* 기타 총 12개 차원

더 흥미로운 부분은 완화 전략인데요, 크게 세 가지입니다.

1. 에피소드 메모리 통합

   * 상호작용 기록을 주기적으로 압축·요약해서\
     오래된 잡음을 줄이고 핵심 패턴만 유지

2. 드리프트 인식 라우팅

   * 에이전트 안정성 점수를 보고\
     특정 에이전트가 불안정하면 라우팅을 바꾸거나 초기화

3. 적응형 행동 고정(behavioral anchoring)

   * 초기에 "이렇게 행동해야 한다"는 예시를 고정점처럼 두고\
     드리프트 정도에 따라 그 가중치를 동적으로 조절

실험 결과, 특히 세 번째 전략이 드리프트를 70% 이상 줄이는 데 효과적이었다고 합니다.

제 입장에서는 이 논문이 **에이전트 시스템 운영의 SRE 가이드 초판** 같은 느낌이었습니다.

* 이제는 "잘 작동하는 에이전트를 만드는 법"뿐 아니라

* "시간이 지나도 망가지지 않게 운영하는 법"이\
  중요한 주제가 되었다는 거죠.



## 🚀 TTT-Discover: 추론 중에 진짜로 학습해서 SOTA를 깨버리는 방식

개인적으로 이번 주 논문 중 가장 흥미로웠던 건 이겁니다.

* **Learning to Discover at Test Time (TTT-Discover)** \
  [https://arxiv.org/abs/2601.16175](https://arxiv.org/abs/2601.16175) \
  코드: [https://github.com/test-time-training/discover](https://github.com/test-time-training/discover)

질문은 아주 단순합니다.

> 특정 과학 문제에서 새로운 SOTA를 LLM이 직접 찾아낼 수 있을까?

기존 접근(예: AlphaEvolve)은 고정된 LLM을 계속 프롬프트해서 검색하는 방식이었습니다.

TTT-Discover는 한 발 더 나아가서,

* 테스트 타임에 **강화학습으로 모델을 실제로 계속 학습**시킵니다.

* 목표는 "많은 문제를 평균적으로 잘 풀기"가 아니라\
  "이 한 문제에서 정말 좋은 솔루션 하나를 찾기"

이게 핵심입니다. 테스트 타임 학습인데, 완전히 **문제 특화형**입니다.

구조적으로는:

* 상태 아카이브: 지금까지 찾은 솔루션 후보와 점수 저장

* 상태 선택 규칙: PUCT 계열 탐색으로 어느 상태를 확장할지 선택

* 보상: 연속 값(성능, 속도 등)을 사용

이걸 다양한 도메인에 적용했는데요.

* 에르되시의 최소 겹침 문제

* GPU 커널 최적화(기존보다 최대 2배 빠름)

* AtCoder 알고리즘 대회 문제

* 단일 세포 분석에서의 노이즈 제거

대부분에서 새 SOTA를 만들어냅니다.\
게다가 사용한 모델도 **오픈 모델 gpt-oss-120b**였다는 게 인상적입니다.

제 입장에서 가장 크게 와닿았던 포인트는 이거였습니다.

> 지금까지 우리는 "테스트 타임 튜닝"을\
> 거의 항상 도메인 일반화를 위한 기법으로 생각해 왔는데,\
> TTT-Discover는 아주 정반대 방향,\
> "한 문제에 끝까지 꽂혀서 더 잘 풀기 위한 기법"으로 사용한다는 점.

이건 실제로 우리가 복잡한 최적화 문제, 설계 문제, 실험 설계 등에 AI를 쓸 때 엄청 강력한 패턴이 될 수 있습니다.



## 📌 마무리: 지금 우리가 챙겨야 할 포인트들

이번 주 논문들을 쭉 훑으면서, 머릿속에 남은 키워드를 정리하면 이런 느낌입니다.

* **조직으로서의 AI**\
  단일 모델이 아니라, 역할·인센티브·감사 구조를 가진 에이전트 조직 설계

* **추론 시점 학습(test-time training)**\
  한 번 학습된 모델을 그냥 쓰는 게 아니라, 문제와 씨름하는 과정에서 계속 배우게 만들기

* **소형 모델(SLM) + 도구 + 오케스트레이션**\
  큰 모델 하나보다, 작고 똑똑한 여러 조각을 잘 조율하기

* **모델 내부 기하학 활용**\
  그래디언트 방향, 스펙트럼, 위치 임베딩 등 "안쪽 구조"를 적극적으로 이용하는 설계

* **운영 관점의 안정성(Agent Drift, ASI)**\
  시스템이 시간에 따라 어떻게 무너지는지, 그리고 그걸 어떻게 막을지

개인적인 조언을 몇 가지로 정리해보면 이렇습니다.

1. **새 프로젝트**를 시작한다면,\
   처음부터 "에이전트 조직" 관점에서 설계해보는 걸 추천합니다.\
   최소한 기획자 / 실행자 / 검수자 정도 레이어는 나눠두면, 나중에 확장이 훨씬 쉬워집니다.

2. **툴 호출이 핵심인 서비스**라면,\
   대형 모델로 직접 툴을 다루기보다\
   300M~1B급 SLM을 툴콜 전용으로 파인튜닝하는 설계를 진지하게 고려해볼 만합니다.\
   위 논문 결과를 보면, 이건 단순한 비용 최적화가 아니라 **성능 최적화**이기도 합니다.

3. **긴 컨텍스트가 중요한 도메인**(법률, 리서치, 코드베이스 요약 등)이라면,\
   DroPE 같은 맥락 확장 기법을 미리 눈여겨보고,\
   나중에 교체 가능한 아키텍처로 만들어두면 좋습니다.

4. **에이전트 시스템**을 만들고 있다면,\
   지금 당장은 ASI 지표를 그대로 구현하지 않더라도,\
   "이 시스템이 시간 지나면 이상해질 수 있다"는 전제를 깔고\
   로그, 메트릭, 리셋 전략을 미리 설계해두는 게 좋습니다.

5. 마지막으로,\
   **정말 어려운 문제 하나를 풀어야 하는 상황이라면**\
   TTT-Discover 스타일의 테스트 타임 강화학습을\
   "현실적인 옵션"으로 진지하게 고려해야 할 시점이 온 것 같습니다.

각 논문이 궁금하다면, 본문 중에 링크 걸어둔 arXiv / GitHub / 프로젝트 페이지들을 한 번씩 눌러보세요.

